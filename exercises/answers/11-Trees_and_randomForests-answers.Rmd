---
title: "Trees and (random)Forests"
author: "Adi Sarid / adi@sarid-ins.co.il"
output: html_document
---

In this chapter we discuss two methods for prediction and regression: trees and random forests.

The general idea of trees is to split the variable space, each time by a different variable, until reaching a "leaf" - an area small enough such that "most" of the observations in it belong to the same class (or with a small variance).

Trees are visually appealing, but they tend to be lousy predictors, hence, a common generalization is a random forest, which generates many trees and then averages them.

```{r fitting a tree to the diamonds data, warning=FALSE, message=FALSE}

library(tidyverse)

ggplot(diamonds, aes(y = price, x = carat)) + 
  facet_wrap(~ clarity) + 
  stat_smooth(method = "lm")

library(rpart)

diamond_price_tree <- rpart(formula = price ~ ., 
                            data = diamonds)

library(rpart.plot)
prp(diamond_price_tree)
diamond_price_tree
summary(diamond_price_tree)
```

As the tree becomes "depper" we're prone to more overfitting errors. Here is an example for a very deep tree (which is probably not very effective).

```{r varying the complexity parameter}
diamond_price_tree_large <- rpart(formula = price ~ ., 
                                  data = diamonds,
                                  control = rpart.control(cp = 0.0005, xval = 10))
prp(diamond_price_tree_large)
#summary(diamond_price_tree_large)

```

A complexity parameter controls the tree's depth. When the paramter is low, the algorithm tends to perform more splits (the CP acts as a split threshold).

To get back to a smaller tree, we can prune the tree, similar to what we did in a step-wise selection algorithm in regression.

How do the algorithms work?

## Growing and prunning trees

The algorithms divide the space of observations into "hyper-planes" (half-spaces) each time, and predicting the target variable according to the resulting division, minimizing:

\[\sum_{j=1}^J\sum_{i\in R_j}\left(y_i-\hat{y}_{R_j}\right)^2\]

Where $j$ is the number of half-spaces dividing the feature space $X$.

At each step of the algorithm the "best split" is examined, looking for the feature and cutpoint which minimizes:

\[\sum_{i: x_i\in R_1(j,s)}\left(y_i-\hat{y}_{R_1}\right)^2 + \sum_{i: x_i\in R_2(j,s)}\left(y_i-\hat{y}_{R_2}\right)^2\]

Where:

\[R_1(j,s) = \left\{X|X_j<s\right\} \text{ and } R_2(j,s) = \left\{X|X_j\geq s\right\}\]

This is what's called a greedy algorithm (at each step looking for current best cutpoint).

### Prunning

To prune a tree we can use `prune`.

```{r pruning a tree}

diamond_price_pruned <- prune(diamond_price_tree_large, cp = 0.05)

prp(diamond_price_pruned)

```

## Using cross-validation to choose the complexity parameter

To choose CP, one can use cross-validation, what cross validation does is:

   * Chooses a CP
   * SPlit the original data to $k$ data sets (k-fold cross validation, $k=10$ is a common choice).
   * For $\frac{k-1}{k}$ of the data fit a tree using the chosen CP.
   * You get $k$ errors, in other words for each CP we get a distribution of errors.
   * Repeat the process for various values of CP.

The `rpart` algorithm actually does all this for us:

```{r example for xvalidation}

# here is the cp table
diamond_price_tree_large$cptable

# the shortest way - use a predefined function to plot the xval cp errors
rpart::plotcp(diamond_price_tree_large)

```

In this case our sample is very large so the x-validation error is monotone decreasing (as the CP **decreases**) usually that is not the case.

So far we discussed regression trees, but what happens when we want to use classification?

To measure the error we use the Gini impurity (instead of RSS):

\[G = \sum_{k=1}^K\hat{p}_{mk}(1-\hat{p}_{mk})\]

Where $\hat{p}_{mk}$ is the proportion of observations in the $m$ half space which has a $k$ classification. The measure is lower as $\hat{p}_{mk}$ is more extreme (closer to 0 or to 1).

```{r plot p time 1-p}

ggplot(tibble(p = seq(0, 1, 0.01)), aes(x = p, y = p*(1-p))) + 
  geom_line() + 
  ylab("G = p*(1-p)") +
  ggtitle("Illustration: Gini impurity will be minimized when p=1 or p=0")

```


Some algorithms use entropy instead:

\[D = -\sum_{k=1}^K{\hat{p}_{mk}\log\hat{p}_{mk}}\]


### Exercise

In this exercise we will use decision trees to predict the probability for churn.

   1. Read the file WA_Fn-UseC_-Telco-Customer-Churn.csv.
   2. Build a decision tree to predict churn using a high CP and another tree using a lower CP. 
   3. Plot the two trees, can you deduce any insights?
   4. Show the cross validation error as a functino of CP. What CP would you choose?
   5. Split the data to train/test and use the CP you got to fit a tree. Plot an ROC based on the train data and the tree you got. 
   6. Fit a logistic regression model to predict churn and compare it to the tree you got (plot both of them on the ROC). Which model has a better performance?

```{r churn using rpart, include = FALSE}

# q1
telco_churn <- read_csv("https://github.com/adisarid/Riskified_training/raw/master/datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv") %>%
  select(-customerID)

# q2
telco_churn_tree <- rpart(data = telco_churn,
                          formula = Churn ~ .,
                          control = rpart.control(cp = 0.001))
# q2+q3
library(rpart.plot)
prp(telco_churn_tree)
telco_churn_short <- prune(telco_churn_tree, cp = 0.01)
prp(telco_churn_short)

# q4
printcp(telco_churn_tree)
plotcp(telco_churn_tree)

# q5
telco_churn <- telco_churn %>%
  mutate(is_train = runif(nrow(telco_churn)) < 0.8)

telco_churn_tree_train <- rpart(data = telco_churn %>% filter(is_train),
                                formula = Churn ~ . - is_train,
                                control = rpart.control(cp = 0.01))

telco_churn_deeptree_train <- rpart(data = telco_churn %>% filter(is_train),
                                formula = Churn ~ . - is_train,
                                control = rpart.control(cp = 0.000001))

# Competitive model using logistic regression
telco_churn_glm_train <- glm(formula = (Churn=="Yes") ~ . - is_train,
                             family = binomial,
                             data = telco_churn %>% filter(is_train))

telco_churn_roc <- telco_churn %>%
  mutate(probability_churn_tree = predict(telco_churn_tree_train, newdata = telco_churn)[, "Yes"]) %>%
  arrange(desc(probability_churn_tree)) %>%
  filter(!is_train) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "tree_cp0.01")

telco_churn_roc_deep <- telco_churn %>%
  mutate(probability_churn_deeptree = 
           predict(telco_churn_deeptree_train, newdata = telco_churn)[, "Yes"]) %>%
  filter(!is_train) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  arrange(desc(probability_churn_deeptree)) %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "tree_cp0.000001")

telco_churn_roc_glm <- telco_churn %>%
  mutate(probability_churn_glm = 
           predict(telco_churn_glm_train, newdata = telco_churn, type = "response")) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  filter(!is_train) %>%
  arrange(desc(probability_churn_glm)) %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "logistic regression")

roc_prep <- telco_churn_roc %>%
  bind_rows(telco_churn_roc_deep,
            telco_churn_roc_glm)
  

ggplot(roc_prep, aes(x = fpr, y = tpr, color = model)) + 
  geom_line() + 
  xlab("False positive rate (1 - Specificity)") + 
  ylab("True positive rate (Sensitivity)") + 
  scale_x_continuous(labels = scales::percent) + 
  scale_y_continuous(labels = scales::percent) +
  ggtitle("An ROC for our churn decision tree model") +
  geom_abline(intercept = 0, slope = 1)


```
   
### Some notes on trees

   * Decision trees are "easier" to explain, thanks to their visual nature.
   * They can express complex relationships ("if-then") that regression cannot represent in a strait-forward manner.
   * Missing values and factor variables are easily handled.
   
But

   * As a prediction model, they are not that good.
   * They are not robust - small changes in the data can lead to an entirely different tree.
   
Many algorithms build upon decision trees. These algorithms are more robust and usually outperform trees:

   * randomForests
   * Bagging
   * Boosting

## randomForests

The random forest algorithm builds an assortment of many trees, but at each split, the number of splitting variables from which the tree building algorithm chooses is limited to $m$ out of possible $p$ (usually $m\approx \sqrt{p}$).

In addition, the algorithm excludes observations at random for the tree building process.

This method can minimize the effects of local minimum caused by the greedy approach of tree building.

The eventual outcome is the prediction average over all trees.

In addition, we can also compute the average contibution of each variable (the decrease in Gini's impurity, that each new variable brings).


```{r diamond random forest}

library(randomForest)

# note the use of maxnodes, otherwise the trees are grown to maximal size
# also limiting the number of trees to 150 - the default is 500...
diamond_price_forest <- randomForest(
  formula = price ~ .,
  data = diamonds,
  maxnodes = 15,
  ntree = 150)

# plot the importance plot
varImpPlot(diamond_price_forest)

# show an example of the first tree
getTree(diamond_price_forest, k = 1)


```

### Exercise

   1. Build a random forest for the Telco churn data. Remember to use a train/test split.
   2. Compute the type-I and type-II error.
   3. Add the results as another ROC line (to the chart you generated in the previous exercise).
      a. Note that when you use the function `predict` you must define the parameter `type` in a specific way. How?
   4. Build an importance plot using `varImpPlot`. What parameters are most important to predict churn?
   5. Would you consider giving a discount to reduce churn? What % of monthlyCharges would you consider as a discount (if any)?
   
```
# some help: 
# first if you try to replicate the code from the diamond's example, you will get an error.
# this is because randomForest expects no character variables, just numeric and factors.
# So how can we turn everything character into factor?
# Intuitively, you would probably do:

# telco_churn <- telco_churn %>%
#    mutate(gender = as.factor(gender),
#           SeniorCitizen = as.factor(SeniorCitizen),
#           ...)

# but this is like doing the same action over and over again.
# wouldn't it be nice to just loop over everything, and if a column is of the wrong type (character)
# just convert it into factor?
# mutate_if() does exactly that.
# it needs a condition called .predicate, and a vector function to operate called .funs. 
# It goes like this:

telco_churn <- telco_churn %>%
   mutate_if(.predicate = funs(typeof(.)=="character"), 
             .funs = funs(as.factor(.)))

# What does it do?
# it's like looping
# for (i in 1:NCOL(telco_churn)){
#    if (typeof(telco_churn[,i]) == "character") {
#       telco_churn[,i] <- as.factor(telco_churn[,i])
#    }
# }

# the syntax of 
# typeof(.) == "character"
# is like "replace . with the vector you are currently checking"
# the syntax of as.factor(.) is like
# "replace . with the vector" that is a character and you need to type case into a factor
# the funs() function makes the expression explicit after replacing the . with the current vector

# now you can continue the exercise...
```
   
```{r example for random forest, include = FALSE}
library(randomForest)

# the randomForest needs preprocessing of characters into factors
telco_churn <- telco_churn %>%
  mutate_if(.predicate = funs(typeof(.) == "character") , funs(as.factor(.)))

# now build the forest, might take a while
churn_forest <- randomForest(formula = factor(Churn) ~ . - is_train,
                             data = telco_churn %>% filter(is_train),
                             importance = TRUE,
                             ntree = 500,
                             na.action = na.omit)

# show the first three trees
head(getTree(churn_forest, k = 1, labelVar = TRUE), 100)
head(getTree(churn_forest, k = 2, labelVar = TRUE), 100)
head(getTree(churn_forest, k = 3, labelVar = TRUE), 100)

# compute the confusion matrix - it uses the classification (majority) rule
telco_churn_predicted <- telco_churn %>%
  mutate(forest_churn_pred = predict(churn_forest, newdata = telco_churn, type = "response")) %>%
  filter(!is_train) %>%
  count(Churn, forest_churn_pred) %>%
  filter(!is.na(forest_churn_pred)) %>%
  group_by(Churn) %>%
  mutate(prop = n/sum(n)) %>%
  select(-n) %>%
  spread(forest_churn_pred, prop)
# type-I error is 10.4%, type-II error is 48.4%.

# variable importance chart
varImpPlot(churn_forest)

# compute prediction and ROC over test set
telco_churn_roc_forest <- telco_churn %>%
  mutate(probability_churn_forest = 
           predict(churn_forest, newdata = telco_churn, type = "prob")[,2]) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  filter(!is_train) %>%
  arrange(desc(probability_churn_forest)) %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "random forest")

roc_prep <- telco_churn_roc %>%
  bind_rows(telco_churn_roc_deep,
            telco_churn_roc_glm,
            telco_churn_roc_forest)

ggplot(roc_prep, aes(x = fpr, y = tpr, color = model)) + 
  geom_line() + 
  xlab("False positive rate (1 - Specificity)") + 
  ylab("True positive rate (Sensitivity)") + 
  scale_x_continuous(labels = scales::percent) + 
  scale_y_continuous(labels = scales::percent) +
  ggtitle("An ROC for our churn detection models") +
  geom_abline(intercept = 0, slope = 1)


# q5 of the question - offering a discount.

# build a new classifier
# omit the TotalCharges variable (tenure and MonthlyCharges already express the total charges)
churn_detection <- glm(telco_churn %>% filter(!is_train),
                       formula = Churn ~ . - is_train -TotalCharges,
                       family = binomial)

# add the predicted variable
telco_discount <- telco_churn %>%
  mutate(predicted_churn = predict(churn_detection, newdata = telco_churn, type = "response")) %>%
  filter(!is_train)

# nominal revenue for the next month:
telco_discount %>% 
  mutate(rev_next_month = ifelse(Churn == "No", MonthlyCharges, (1-predicted_churn)*MonthlyCharges)) %>%
  summarize(sum(rev_next_month))

# build a function based on revenue for the next month and activation of discount rate, for the 
# observations about the discount quantile

calculate_predicted_rev <- function(discount_rate = 0.15, discount_quantile = 0.75){
  pred_quantile <- as.numeric(quantile(telco_discount$predicted_churn, discount_quantile))

  telco_discount_comp <- telco_discount %>% 
    mutate(give_discount = predicted_churn >= pred_quantile) %>%
    mutate(originalMonthlyCharges = MonthlyCharges) %>%
    mutate(MonthlyCharges = 
             ifelse(give_discount, originalMonthlyCharges*(1-discount_rate), originalMonthlyCharges)) %>%
    mutate(rev_next_month = ifelse(Churn == "No", MonthlyCharges, (1-predicted_churn)*MonthlyCharges)) %>%
    summarize(rev = sum(rev_next_month))
  
  return(telco_discount_comp$rev)
}

# now, create a grid with the function running on different combinations
# I'm going to use pmap to generate the results via functional programming,
# but this can also be accomplished by looping over lines of tibble
discount_options <- as.tibble(
  expand.grid(discount_rate = seq(0, 0.2, 0.025), 
              discount_quantile = seq(0.5, 0.95, 0.05))) %>%
  mutate(rev = map2_dbl(discount_rate, discount_quantile, `calculate_predicted_rev`))

ggplot(discount_options, aes(x = discount_rate, y = discount_quantile, fill = rev)) + 
  geom_raster(interpolate = TRUE) + 
  geom_contour(aes(z = rev), color = "yellow", size = 1.5) + 
  ggtitle("The expected revenue as a function of discount rate and discount quantile\n(above which churn likelihood quantile to apply it)")
  
```

## גישת ה-Boosting

ביערות אקראיים ראינו כיצד חזאי בודד (עץ) משתכפל והופך לשילוב של הרבה חזאים. כאשר מבוצע שילוב של חזאים רבים הדבר מהווה פוטנציאל להפחית שגיאות שעשויות להופיע באופן מקומי בחזאים אשר נתונים פעמים רבות ל"גחמות הסטטיסטיקה" (או שגיאות הנובעות ממינימום מקומי או מהתאמת יתר).

גישה נוספת חוץ מיערות אקראיים היא גישת ה-Boosting. היא יכולה להתאים לגישות שונות (לאו דווקא להכללה של עצים), אך פה נדגים אותה בהקשר העצים.

נניח שהבעיה שלנו היא בעיית רגרסיה (חיזוי ערך של משתנה רציף). ב-Boosting, בכל שלב האלגוריתם יבנה עץ, שהמטרה שלו היא חיזוי השגיאה (לא הערך האמיתי של $y$ אלא השגיאה הצפוייה בהתבסס על כל העצים שנבנו עד כה).

המודל מתווסף כסכום לכל יתר המודל שחושבו עד כה, עם פרמטר "הקטנה" $\lambda$.

במילים אחרות, האלגוריתם מרכיב סכום של הרבה עצים קטנים, כשכל פעם הוא נותן דגש על צמצום השגיאות שהתקבלו עד כה.

```
Pseudo code:
Set r = y, f(x)=0
For b=1, 2,..., B repeat:
   Fit a tree, f_curr, to the data (X,r)
   Update f by adding current learned tree: 
      f <- f + lambda*f_curr
   Update the residuals
      r - r-lambda*f_curr
Output f
```

במקרה של בעיות סיווג, העדכון של המודלים מתבצע על ידי בניה בכל שלב של מודל סיווג (לדוגמה עץ), תוך כדי מתן משקל גדול יותר לתצפיות אשר הסיווג שלהן שגוי.

ב-R יש שתי חבילות המשמשות לboosting:

   * adabag
   * xgboost

### תרגיל

השתמשו בחבילת xgboost, בפקודת `xgboost` כדי לייצר חזאי לנטישת לקוחות.
הצגת התוצאות בתרשים ה-ROC. האם ביצוע boosting שיפר את החזאים?

שימו לב, פקודת `xgboost` דורשת הכנה של מבנה הנתונים למטריצה. הנה קוד שיסייע לכם בהכנת המטריצה. השלימו את ה-XXX עם הפקודות / שמות משתנים המתאימים. שימו לב לשימוש ב-`mutate_all`, הדומה לפקודה שהשתמשנו בה קודם `mutate_if`.

```
# prepare the data
telco_churn_for_boost <- telco_churn %>%
  filter(is_train) %>%
  select(XXX:XXX)

dtrain <- xgb.DMatrix(telco_churn_for_boost %>%
                        mutate_all(funs(as.numeric(XXX))) %>%
                        select(-XXX) %>%
                        as.matrix(), 
                      label = XXX == XXX)

# building the boost predictor
churn_boost <- xgboost::xgboost(data = dtrain, 
                                nrounds = XXX, 
                                params = 
                                  list(objective=XXX,
                                       booster=XXX))

```

האם יש פרמטרים של העצים הנבנים במהלך אלגוריתם ה-boosting שאתם יכולים לשנות, כך שישפרו את החזאי?

```{r boosting example using xgboost, include=FALSE}
library(xgboost)

telco_churn_for_boost <- telco_churn %>%
  filter(is_train) %>%
  select(gender:Churn)

dtrain <- xgb.DMatrix(telco_churn_for_boost %>%
                        mutate_all(funs(as.numeric(.))) %>%
                        select(-Churn) %>%
                        as.matrix(), 
                      label = telco_churn_for_boost$Churn == "Yes")

churn_boost <- xgboost::xgboost(data = dtrain, nrounds = 1000, 
                                params = 
                                  list(objective="binary:logistic",
                                       booster="gbtree"))

churn_boost_predict <- telco_churn %>%
  select(gender:TotalCharges) %>%
  mutate_all(funs(as.numeric(.))) %>%
  as.matrix() %>%
  predict(object = churn_boost, newdata = .)


# compute prediction and ROC over test set
telco_churn_roc_boost <- telco_churn %>%
  mutate(probability_churn_boost = churn_boost_predict) %>%
  mutate(churn_numeric = Churn == "Yes") %>%
  filter(!is_train) %>%
  arrange(desc(probability_churn_boost)) %>%
  mutate(tpr=cumsum(churn_numeric)/sum(churn_numeric),
         fpr=cumsum(!churn_numeric)/sum(!churn_numeric)) %>%
  mutate(model = "boosting")

roc_prep <- telco_churn_roc %>%
  bind_rows(telco_churn_roc_deep,
            telco_churn_roc_glm,
            telco_churn_roc_forest,
            telco_churn_roc_boost)

ggplot(roc_prep, aes(x = fpr, y = tpr, color = model)) + 
  geom_line() + 
  xlab("False positive rate (1 - Specificity)") + 
  ylab("True positive rate (Sensitivity)") + 
  scale_x_continuous(labels = scales::percent) + 
  scale_y_continuous(labels = scales::percent) +
  ggtitle("An ROC for our churn detection models") +
  geom_abline(intercept = 0, slope = 1)


```